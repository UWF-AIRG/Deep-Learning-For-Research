{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeployingModels.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8LDqp4HfVjw",
        "colab_type": "text"
      },
      "source": [
        "# Model Deployment:\n",
        "\n",
        "Today we will be going over an easier way to get your models into production using Render. Render is used by most of the Fast.AI students to get their models up, and we can run the files locally ourselves to visually see it. To do this, I have provided download links to all of the standard models Fast.AI have used. \n",
        "\n",
        "We will explore productioninzing the following models:\n",
        "* **Computer Vision:** Cats vs Dogs\n",
        "* **Tabular:** > or <= $50k\n",
        "* **NLP:** IMDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfGpxHKbga3t",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://course.fast.ai/images/render/render-logo.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq8NC2ZAglcT",
        "colab_type": "text"
      },
      "source": [
        "To use the Render template, you will need docker installed. You can run the following below to get it running locally and test your changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWDp94yMfQFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docker build -t fastai-v3 . && docker run --rm -it -p 5000:5000 fastai-v3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1j9qe0Ce6HM",
        "colab_type": "text"
      },
      "source": [
        "**OR** Simply run `python3 app/server.py serve`.\n",
        "\n",
        "Before you do this though, do a `pip3 install -r requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAgi1K5Ue4OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "* Or simply run "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4KiwVwyhhqu",
        "colab_type": "text"
      },
      "source": [
        "## Repo\n",
        "\n",
        "Jeremy has a render example repo that we will be working off of [here](https://github.com/render-examples/fastai-v3). First thing we need is our models. Let's look at the Pets notebook first!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sleaLlyj9FyM",
        "colab_type": "text"
      },
      "source": [
        "## Cats vs Dogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCCC3-W49IRK",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "This should take ~5 minutes to run on your own. For todays purposes though we *just* want the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkGZhLqW8_XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.vision import *\n",
        "path = untar_data(URLs.PETS)\n",
        "path_img = path/'images'\n",
        "fnames = get_image_files(path_img)\n",
        "np.random.seed(2)\n",
        "pat = re.compile(r'/([^/]+)_\\d+.jpg$')\n",
        "data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224)\n",
        "data = data.normalize(imagenet_stats);\n",
        "learn = create_cnn(data, models.resnet34, pretrained=True, metrics=error_rate)\n",
        "learn.fit_one_cycle(2);\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmtLPnNf_aei",
        "colab_type": "text"
      },
      "source": [
        "### Exporting the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8ez7Vh6-6Ah",
        "colab_type": "code",
        "outputId": "0b6c943f-1f34-46ce-e55c-868799148d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "learn.path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.fastai/data/oxford-iiit-pet/images')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pil1gpVQ_JpM",
        "colab_type": "text"
      },
      "source": [
        "Well, that's not very easy to get to. Let's change that to Colab's root working directory!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zSl6cJW9jhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.path = Path('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YypSD1ii_QzA",
        "colab_type": "text"
      },
      "source": [
        "Now we can run `learn.export()` and get a pkl file with everything we need! (This will also change where learn.save() points to as well)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxlboB81_P6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.export('pets.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWcKsQpAMsQW",
        "colab_type": "text"
      },
      "source": [
        "**NOTE** When we want to load the model, we can now run `load_learner(path, fname)`. This is **different** than a simple `learn.save()` and `learn.load()` combination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v8KrpkN_9ha",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the model\n",
        "\n",
        "To download my already-built model, run the following"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS_qaHfDAlBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1oIn2_DxTJIYBWofQFOQQFcyO8yxJ3H8G' -O 'pets.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5bFtXHxBN4D",
        "colab_type": "text"
      },
      "source": [
        "### Modifying the files we need to\n",
        "\n",
        "Most of what we will be modifying is the 'server.py' file, which is inside the `app` directory. Now first, I recommend either uploading that file to your own google drive, or to dropbox. Then (if you use google drive for your model) change your sharing settings to 'Public', copy the share URL, and find the 'ID=' section. Copy that bit, and replace 'YOURID' in the below:\n",
        "\n",
        "'https://docs.google.com/uc?export=download&id=YOURID'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iurgq_NvB9QH",
        "colab_type": "text"
      },
      "source": [
        "Now let's look at the server.py. I'll post the whole thing below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JB60N-QAnTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "import uvicorn\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from io import BytesIO\n",
        "from starlette.applications import Starlette\n",
        "from starlette.middleware.cors import CORSMiddleware\n",
        "from starlette.responses import HTMLResponse, JSONResponse\n",
        "from starlette.staticfiles import StaticFiles\n",
        "\n",
        "export_file_url = 'https://www.dropbox.com/s/6bgq8t6yextloqp/export.pkl?raw=1'\n",
        "export_file_name = 'export.pkl'\n",
        "\n",
        "classes = ['black', 'grizzly', 'teddys']\n",
        "path = Path(__file__).parent\n",
        "\n",
        "app = Starlette()\n",
        "app.add_middleware(CORSMiddleware, allow_origins=['*'], allow_headers=['X-Requested-With', 'Content-Type'])\n",
        "app.mount('/static', StaticFiles(directory='app/static'))\n",
        "\n",
        "\n",
        "async def download_file(url, dest):\n",
        "    if dest.exists(): return\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(url) as response:\n",
        "            data = await response.read()\n",
        "            with open(dest, 'wb') as f:\n",
        "                f.write(data)\n",
        "\n",
        "\n",
        "async def setup_learner():\n",
        "    await download_file(export_file_url, path / export_file_name)\n",
        "    try:\n",
        "        learn = load_learner(path, export_file_name)\n",
        "        return learn\n",
        "    except RuntimeError as e:\n",
        "        if len(e.args) > 0 and 'CPU-only machine' in e.args[0]:\n",
        "            print(e)\n",
        "            message = \"\\n\\nThis model was trained with an old version of fastai and will not work in a CPU environment.\\n\\nPlease update the fastai library in your training environment and export your model again.\\n\\nSee instructions for 'Returning to work' at https://course.fast.ai.\"\n",
        "            raise RuntimeError(message)\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "\n",
        "loop = asyncio.get_event_loop()\n",
        "tasks = [asyncio.ensure_future(setup_learner())]\n",
        "learn = loop.run_until_complete(asyncio.gather(*tasks))[0]\n",
        "loop.close()\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "async def homepage(request):\n",
        "    html_file = path / 'view' / 'index.html'\n",
        "    return HTMLResponse(html_file.open().read())\n",
        "\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "async def analyze(request):\n",
        "    return JSONResponse({'result': str(prediction)})\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if 'serve' in sys.argv:\n",
        "        uvicorn.run(app=app, host='0.0.0.0', port=5000, log_level=\"info\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZjenLyaCFOL",
        "colab_type": "text"
      },
      "source": [
        "Wow! Lot's to unpack here! Let's go through it bit by bit.\n",
        "\n",
        "For our app to work, we need the following libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTkiytmmCOCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "import uvicorn\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from io import BytesIO\n",
        "from starlette.applications import Starlette\n",
        "from starlette.middleware.cors import CORSMiddleware\n",
        "from starlette.responses import HTMLResponse, JSONResponse\n",
        "from starlette.staticfiles import StaticFiles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3JaimLiCOf9",
        "colab_type": "text"
      },
      "source": [
        "* [aiohttp](https://aiohttp.readthedocs.io/en/stable/): HTTP client/server for asyncio\n",
        "* [asyncio](https://docs.python.org/3/library/asyncio.html): A python framework for good IO performances\n",
        "* fastai - We *need* the library for our model\n",
        "* [starlette](https://www.starlette.io/): \"A lightweight ASGI framework for high performance asyncio services\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlGX9T84FYSH",
        "colab_type": "text"
      },
      "source": [
        "### The *non* functions:\n",
        "\n",
        "By non-functions I mean the base variables. As you can see, we have a few things. First, we need a link to download our model, then some filename to give it, and lastly what classes we expect things to fall into."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcTwyU5TIeGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "export_file_url = 'https://www.dropbox.com/s/6bgq8t6yextloqp/export.pkl?raw=1'\n",
        "export_file_name = 'export.pkl'\n",
        "\n",
        "classes = ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair',\n",
        " 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue',\n",
        " 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier',\n",
        " 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel',\n",
        " 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese',\n",
        " 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher',\n",
        " 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed',\n",
        " 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier',\n",
        " 'yorkshire_terrier']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seOo5-0kDXMJ",
        "colab_type": "text"
      },
      "source": [
        "### The Functions:\n",
        "\n",
        "We have a few functions, 'analyze', 'homepage', 'download_files', and 'setup_learner'. They all do pretty much what we need to get our model' up and running. Analyze we can adjust to however we want our input data to come in. In our case, we expect the user to upload an image file, ad we extract those bytes, utilize the `open_image` function, and run `learn.predict()`. This will be one of the main functions we will change for our use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXjmtEydExFt",
        "colab_type": "text"
      },
      "source": [
        "Now what is an app_route? Those mean seperate web-pages on our server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaNvUgDtFAJS",
        "colab_type": "text"
      },
      "source": [
        "#### analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeGczuyMDAFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@app.route('/analyze', methods=['POST'])\n",
        "async def analyze(request):\n",
        "    img_data = await request.form()\n",
        "    img_bytes = await (img_data['file'].read())\n",
        "    img = open_image(BytesIO(img_bytes))\n",
        "    prediction = learn.predict(img)[0]\n",
        "    return JSONResponse({'result': str(prediction)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mig9fKCNGvvC",
        "colab_type": "text"
      },
      "source": [
        "#### setup_learner and download_file\n",
        "\n",
        "We don't really need to mess with these as they do exactly what we need, regardless of the model, and they're pretty self-explanitory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9VfgilwHa3x",
        "colab_type": "text"
      },
      "source": [
        "## Running our server!\n",
        "\n",
        "To run your server locally, navigate to your directory you cloned and run the following **if** you have Docker installed:\n",
        "\n",
        "`docker build -t fastai-v3 . && docker run --rm -it -p 5000:5000 fastai-v3`\n",
        "\n",
        "Else run: `python app/serve.py serve`\n",
        "\n",
        "Now we're running on http://localhost:5000/ !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLNGjNPlH6ps",
        "colab_type": "text"
      },
      "source": [
        "Try it! Currently the HTML page is set up for Jeremy's lesson 2 where he built a bear classifier, but in our lesson today I won't go over HTML and web-page development, you guys can get that experience on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj9qUBngJgYC",
        "colab_type": "text"
      },
      "source": [
        "# Other Model Types\n",
        "\n",
        "The next one we will look at is NLP-based models. I made an app for CodeFest last year called 'Suspecto' where we utilized an NLP model to help grade a models reliability. Using the standard Fast.AI approach we won first place. We'll borrow some of the website code for that setup today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOPNpeUzKIIS",
        "colab_type": "text"
      },
      "source": [
        "## What do we change?\n",
        "\n",
        "First, change the imports to use whatever libraries you used. In my case, this involved the `fastai.text` library.\n",
        "\n",
        "Next, we changed analyze. Here, we wanted our model to take the results from `learn.predict()`, and based on a formula we had made as a reliability score, report this back to the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsqhtYNOKgqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@app.route('/analyze', methods=['POST'])\n",
        "async def analyze(request):\n",
        "    data = await request.form()\n",
        "    content = data['content']\n",
        "    prediction = learn.predict(content)[2]\n",
        "    reliability = prediction[7]-(prediction[6]*prediction[0]) - prediction[0] - prediction[4] - prediction[5] - prediction[8] - (prediction[1]-prediction[11])\n",
        "    ReliabilityScore = ((reliability.item())*50)+50\n",
        "    ReliabilityScore = int(ReliabilityScore)\n",
        "    return JSONResponse({'result': ReliabilityScore})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh-TomxzKsz0",
        "colab_type": "text"
      },
      "source": [
        "Here, notice instead we get ['content'] instead. This is due to our request form changing to a text box. To do this, we adjusted index.html. You see we have a special 'content' checker? This lives inside:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZw2xQr2LheQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "<div class='content'>\n",
        "    <textarea id=\"content-upload\" cols=\"85\" rows=\"15\"></textarea>\n",
        "    <div class='analyze'>\n",
        "        <button id='analyze-button' class='analyze-button' type='button' onclick='analyze()'>Analyze</button>\n",
        "    </div>\n",
        "    <div class='result-label'>\n",
        "        <label id='result-label'></label>\n",
        "    </div>\n",
        "</div>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xma07FLCLlHf",
        "colab_type": "text"
      },
      "source": [
        "So depending on what you want, see what the HTML equivalent is, and so long as you put it in, you can pass it to our analyze function! Now in our case, we want to instead do a movie review, so let's modify a few things!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26_5nXaAL0CE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@app.route('/analyze', methods=['POST'])\n",
        "async def analyze(request):\n",
        "    data = await request.form()\n",
        "    content = data['content']\n",
        "    prediction = learn.predict(content)[0]\n",
        "    return JSONResponse({'Review Rating': str(prediction)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7ZMBQXfL-8l",
        "colab_type": "text"
      },
      "source": [
        "Looks pretty close to what we had before, doesn't it! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ionPEPDgMERm",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Ideas and Tabular Production\n",
        "\n",
        "\n",
        "\n",
        "In the *real* world, we need to ask a few questions. How will I get my data? How will it be easiest for my customer to send me their data? For example. Take the image problem. If I am dealing with large sets of data they want me to analyze, I'm not going to expect my customer to sit there and upload 100's of files one by one and then we run them! That would be absurd. Instead, we can tell them to provide us with links to the relevant image websites they want us to run predictions with in the form of a CSV, *or* upload a zip document that contains all of their images into a nice folder structure that we can specify.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdN-qLi0OodA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import csv\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "async def analyze(request):\n",
        "    data = await request.form()\n",
        "    content = data['content']\n",
        "    zip_ref = zipfile.ZipFile(content, 'r')\n",
        "    mkdir('Downloaded_Images')\n",
        "    zip_ref.extractall('Downloaded_Images')\n",
        "    zip_ref.close()\n",
        "    path2 = Path('Downloaded_Images')\n",
        "    data = ImageList.from_folder(path)\n",
        "    learn = load_learner(path, export_file_name, test=data)\n",
        "    y, _ = learn.get_preds(DatasetType.Test)\n",
        "    y = torch.argmax(y, dim=1)\n",
        "    preds = [learn.data.classes[int(x)] for x in y]\n",
        "    rm -r 'Downloaded_Images'\n",
        "    resultsFile = open('results.csv', 'wb')\n",
        "    wr = csv.writer(resultsFile)\n",
        "    wr.writerows([preds])\n",
        "    return FileResponse('results.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FttEfV4WPeQT",
        "colab_type": "text"
      },
      "source": [
        "Now, lets parse this CSV when uploaded and download all of our images. We're going to use the `download_images()` function back from lesson 2 to help with this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnrmj7-Nb575",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import StringIO\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "async def analyze(request):\n",
        "    data = await request.form()\n",
        "    content = await (data['file'].read())\n",
        "    s = str(content, 'utf-8')\n",
        "    data = StringIO(s)\n",
        "    !mkdir('Downloaded_Images')\n",
        "    download_images(data, 'Downloaded_Images')\n",
        "    path2 = Path('Downloaded_Images')\n",
        "    data = ImageList.from_folder(path)\n",
        "    learn = load_learner(path, export_file_name, test=data)\n",
        "    y, _ = learn.get_preds(DatasetType.Test)\n",
        "    y = torch.argmax(y, dim=1)\n",
        "    preds = [learn.data.classes[int(x)] for x in y]\n",
        "    rm -r 'Downloaded_Images'\n",
        "    resultsFile = open('results.csv', 'wb')\n",
        "    wr = csv.writer(resultsFile)\n",
        "    wr.writerows([preds])\n",
        "    return FileResponse('results.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8mqJZ7dOvDu",
        "colab_type": "text"
      },
      "source": [
        "Now, tabular is a pretty special case. In the business world, most work will be done by sending in large chunks of data for analysis on a served model *somewhere*. The company may have it hooked into a GPU to account for the faster time, and having it shut off and on based on when the server gets a request. The models we export are generally CPU based models, but we can adjust for this if needed.\n",
        "\n",
        "Now let's recreate what we did above for tabular data. Instead we will load it into pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNuwpWKGeMpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import StringIO\n",
        "import csv\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "async def analyze(request):\n",
        "    data = await request.form()\n",
        "    content = await (data['file'].read())\n",
        "    s = str(content, 'utf-8')\n",
        "    data = StringIO(s)\n",
        "    df = pd.read_csv(data)\n",
        "    data = TabularList.from_df(df, path='', cat_names = cat_names,\n",
        "                              cont_names = cont_names, procs = procs)\n",
        "    learn = load_learner(path, export_file_name, test=data)\n",
        "    y, _ = learn.get_preds(DatasetType.Test)\n",
        "    y = torch.argmax(y, dim=1)\n",
        "    preds = [learn.data.classes[int(x)] for x in y]\n",
        "    df['Predictions'] = preds\n",
        "    \n",
        "    path3 = Path('app/static/')\n",
        "    df.to_csv(path3/'results.csv')\n",
        "    \n",
        "    return FileRespose('results.csv', media_type='csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hUg6nIEeohZ",
        "colab_type": "text"
      },
      "source": [
        "We also need to adjust our JavaScript a little bit too. Specifically the end bit of the Anlyze function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWBTJ3cgex2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  xhr.onload = function(e) {\n",
        "    if (this.readyState === 4) {\n",
        "      el(\"result-label\").innerHTML = `Result = Good`;\n",
        "      \n",
        "      download('results.csv', 'results.csv');\n",
        "      xhr.send();\n",
        "    }\n",
        "    el(\"analyze-button\").innerHTML = \"Analyze\";\n",
        "  };\n",
        "\n",
        "  var fileData = new FormData();\n",
        "  fileData.append(\"file\", uploadFiles[0]);\n",
        "  xhr.send(fileData);\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io4oP-QBey6j",
        "colab_type": "text"
      },
      "source": [
        "As we are now taking FormData"
      ]
    }
  ]
}