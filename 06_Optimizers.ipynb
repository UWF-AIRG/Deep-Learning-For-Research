{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimizations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zfS6BYquuXH",
        "colab_type": "text"
      },
      "source": [
        "<h1> What is <b>Gradient Decent</b>?</h1>\n",
        "\n",
        "<h2>\n",
        "  \n",
        "  \"Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\"\n",
        "  </h2>\n",
        "  \n",
        "  \n",
        "  https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezCoQWGGsOip",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*WGHn1L4NveQ85nn3o7Dd2g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ShYGxTnsm-z",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://camo.githubusercontent.com/203d02ce525466161d67c695e7912a364e16a015/68747470733a2f2f692e737461636b2e696d6775722e636f6d2f316f6274562e676966)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TakJDq7t1T4",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://blog.algorithmia.com/wp-content/uploads/2018/05/word-image-2-1024x479.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdYCVE7Jt62q",
        "colab_type": "text"
      },
      "source": [
        "<h2>Stochastic Gradient Decent (SGD)</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MdO2vVqt-15",
        "colab_type": "text"
      },
      "source": [
        "Instead of calculating the gradient (loss) for *all* training examples, do it on a subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_kci8JTuG-M",
        "colab_type": "text"
      },
      "source": [
        "<h2>Adagrad</h2>\n",
        "\n",
        "Dispenses weights to different individual features in the dataset. Issues: learning rate gets *really* small over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYO5zyGwuRGl",
        "colab_type": "text"
      },
      "source": [
        "<h2> RMSprop</h2>\n",
        "Special version of Adagrad. Instead of letting all gradients accumulate for momentum, only does so in a fixed window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf6tZSf8uY4w",
        "colab_type": "text"
      },
      "source": [
        "<h2>Adam</h2>\n",
        "ADAM: Adaptive Moment Estimation, uses past gradients to calculate current gradients. Also utilizes the concept of momentum by adding fractions of previous gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzSEoIbFvKMG",
        "colab_type": "text"
      },
      "source": [
        "<h2>AdamW</h2>\n",
        "\n",
        "![alt text](https://www.fast.ai/images/adamw_charts.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jjtXM6gvdcT",
        "colab_type": "text"
      },
      "source": [
        "## Properly tuned, Adam really works! We got new state of the art results (in terms of training time) on various tasks like\n",
        "\n",
        "\n",
        "### *   training CIFAR10 to >94% accuracy in as few as 18 epochs with Test Time Augmentation or with 30 epochs without, as in the DAWNBench competition;\n",
        "### *   fine-tuning Resnet50 to 90% accuracy on the Cars Stanford Dataset in just 60 epochs (previous reports to the same accuracy used 600);\n",
        "### * training from scratch an AWD LSTM or QRNN in 90 epochs (or 1 hour and a half on a single GPU) to state-of-the-art perplexity on Wikitext-2 (previous reports used 750 for LSTMs, 500 for QRNNs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOmabee7vxB9",
        "colab_type": "text"
      },
      "source": [
        "## That means that we’ve seen (for the first time we’re aware of) super convergence using Adam! Super convergence is a phenomenon that occurs when training a neural net with high learning rates, growing for half the training. Before it was understood, training CIFAR10 to 94% accuracy took about 100 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1oMxOigvx-b",
        "colab_type": "text"
      },
      "source": [
        "## In contrast to previous work, we see Adam getting about as good accuracy as SGD+Momentum on every CNN image problem we’ve tried it on, as long as it’s properly tuned, and it’s nearly always a bit faster too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S6E47bDvzzF",
        "colab_type": "text"
      },
      "source": [
        "## The suggestions that amsgrad are a poor “fix” are correct. We consistently found that amsgrad didn’t achieve any gain in accuracy (or other relevant metric) than plain Adam/AdamW."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDE6D6Vlukda",
        "colab_type": "text"
      },
      "source": [
        "All have the same goal: Minimize the Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlYP38Jlr38I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.tabular import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwLkQtHAu_0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}