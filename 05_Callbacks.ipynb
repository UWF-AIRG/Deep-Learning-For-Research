{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CallBacks.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["GBiBDXdPXKpv","6Dnk5fpjZafV","yOmj0ITGZ-R2"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"temuo4OuSodZ","colab_type":"text"},"source":["# Week 5, Callbacks"]},{"cell_type":"markdown","metadata":{"id":"eqZlXmgdcNSl","colab_type":"text"},"source":["<h2>Schedule for the next few weeks:</h2>\n","\n","**Week 5:** Callbacks\n","\n","**Week 6:** Optimizers\n","\n","**Week 7:** Loss Functions\n","\n","**Week 8:** Custom Architecture Introduction"]},{"cell_type":"markdown","metadata":{"id":"i4ogQYd1TMex","colab_type":"text"},"source":["<h3> <b>Key Terms and Definitions:</b> </h3>\n","\n","**Model**:  'A mathematical representation of a real-world-process,' in our case consists of an architecture and we call these Deep Learning Models\n","\n","**Loss**: The loss function associated with grading performance to the model.\n","\n","**Metric**: How we grade the workings of our model in a 'pretty' way. Eg: accuracy vs Cross Entropy Loss\n","\n","**Gradient Decent**: Optimization algorithm used to update the parameters of our model, help 'fit'\n","\n","**Callback**: A function that occurs in case something happens\n"]},{"cell_type":"markdown","metadata":{"id":"WCSvb6iWS-0g","colab_type":"text"},"source":["Everything in Fast.AI is done during the training loop. Every single step inside of it can be altered via a callback.\n","\n","<h2>Examples of useful callbacks, some you already know!</h2>\n","\n","*   <b>OneCycleScheduler</b>\n","\n","*   <b>LRFinder</b>\n","\n","* SaveModelCallback\n","* CSVLogger\n","* MixedPrecision\n","\n","Today we are going to go through how each of these work, what it's like building callbacks within fastai, and what all can be done\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ioTFxNhdWkhB","colab_type":"text"},"source":["<h2> The basic training loop </h2>"]},{"cell_type":"markdown","metadata":{"id":"cXmaYCdWWdWw","colab_type":"text"},"source":["Here is the code"]},{"cell_type":"code","metadata":{"id":"DGng3fOOSmSd","colab_type":"code","colab":{}},"source":["# train_dl is the dataloader for your training set in the databunch. \n","#      It is a link to where your databunch set is saved\n","# opt_fn is the optimization function, there are a few. We use Adam.\n","#      Standard Gradient Decent is also commonly found in papers and models\n","def train(train_dl, epochs, opt_fn, loss_func):\n","  for _ in range(epoch):\n","    # Well what is _? _ is a placeholder for some increment we dont care about modifying, \n","    # see Code Examples section for more\n","    model.train() # set the model to train mode\n","    for xb,yb in train_dl: # for each batch of x and y in the dataset\n","      out = model(xb) # This is our forward pass, we get our models output\n","      loss = loss_func(out, yb) # We take the models output and our label and grade\n","      loss.backward() # Our model's backward pass, accumulates gradients (graph)\n","      opt_fn.step() # Updates a parameter on the current gradient\n","      opt_fn.zero_grad() # zeros our gradients"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZjNFBkHMWoQh","colab_type":"text"},"source":["<h2>Pytorch for comparison</h2>"]},{"cell_type":"code","metadata":{"id":"8_UJlOtyWq_6","colab_type":"code","colab":{}},"source":["def pyTrain(train_dl, model, opt_func, loss_func):\n","  losses = AverageMeter('Loss', ':.4e') # how we want to see the losses\n","  top1 = AverageMeter('Acc@1', ':6.2f')\n","  top5 = AverageMeter('Acc@5, '6.2f)\n","  \n","  model.train() # set model to train\n","  for i, (images, target) in enumerate(train_dl): # for every item in the training loader\n","    target = target.cuda(args.gpu, non_blocking=True) # load the target tensor to the GPU\n","    \n","    output = model(images) # compute output and loss\n","    loss = criterion(output, target)\n","    \n","    acc1, acc5 = accuracy(output, target, topk=(1,5)) # topk returns top 'x' predictions\n","    losses.update(loss.item(), images.size(0))\n","    top1.update(acc1[0], images.size(0))\n","    top5.update(acc5[0], images.size(0))\n","    \n","    \n","    optimizer.zero_grad() # zero our gradient\n","    loss.backward() # backward pass\n","    optimizer.step() # Update parameters"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9FDoSVmgS5O-","colab_type":"text"},"source":["![alt text](https://pouannes.github.io/fastai-callbacks/fastai_training_loop_vanilla.png#center)"]},{"cell_type":"markdown","metadata":{"id":"3Z7o3TafYKKZ","colab_type":"text"},"source":["Now lets view where the callback functionalities can apply"]},{"cell_type":"code","metadata":{"id":"8Ohxf6ZxYPJb","colab_type":"code","colab":{}},"source":["def train(train_dl, epoch, opt_fn, loss_func):\n","  callbacks.on_train_begin() # we can call even before we do any epochs\n","  \n","  for _ in range(epochs):\n","    callbacks.on_epoch_begin() # we can call here\n","    model.train()\n","    \n","    for xb,yb in train_dl:\n","      callbacks.on_batch_begin() # even before we get our output!\n","      out = model(xb)\n","      \n","      callbacks.on_loss_begin() # here we have the output and need to modify it\n","      loss = loss_func(out, yb)\n","      \n","      callback.on_backward_begin() # our start of backward pass\n","      loss.backward()\n","      callbacks.on_backward_end() # end of our backward pass\n","      \n","      opt_fn.step()\n","      callbacks.on_step_end() # after we update a parameter\n","      \n","      opt.zero_grad()\n","      callbacks.on_batch_end() # after we finish a batch\n","      \n","    callbacks.on_epoch_end() # end of our epoch\n","    \n","  callbacks.on_train_end() # end of our training"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-HIrIiQvZPOz","colab_type":"text"},"source":["![alt text](https://pouannes.github.io/fastai-callbacks/fastai_training_loop_callbacks.png#center)"]},{"cell_type":"markdown","metadata":{"id":"SS8STNQcZzae","colab_type":"text"},"source":["<h2> Callback Handler </h2>\n","\n","We can do most of whatever we want but we're missing something. How do they handle ANYTHING without access to the state of training itself? (see they're around but not within?)\n","\n","Welcome to the <b>CallbackHandler</b>\n","\n","The CallbackHandler takes relevent data and transmits it to the callbacks and returns value depending on behaviour"]},{"cell_type":"markdown","metadata":{"id":"6yPCWwnaafbV","colab_type":"text"},"source":["Let's have an example. We can use callbacks to send the new training data, *xb* and and *yb* for the batch.\n","Here we will have a CallbackHandler called cb_handler:"]},{"cell_type":"code","metadata":{"id":"Ec3JBX7zaV3x","colab_type":"code","colab":{}},"source":["cb_handler.on_batch_begin(xb,yb)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-wS0bQZat_4","colab_type":"text"},"source":["Now lets think of another example. I want to skip the backward pass if my loss is too high. Also, if my loss is too small, perhaps scale it up!"]},{"cell_type":"code","metadata":{"id":"VAiAMOVJa3JP","colab_type":"code","colab":{}},"source":["loss, skip_backward = cb_handler.on_backward_begin(loss)\n","if not skip_backward: loss.backward() # if we decide we don't want to skip backward"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXfhiy4IbLCV","colab_type":"text"},"source":["The new training loop"]},{"cell_type":"code","metadata":{"id":"V8I5n5rJbNFL","colab_type":"code","colab":{}},"source":["def train(learn, epochs, callbacks, metrics):\n","  cb_handler = CallbackHandler(callbacks)\n","  cb_handler.on_train_begin(epochs, learn, metrics)\n","  \n","  for epoch in range(epochs):\n","    learn.model.train()\n","    cb_handler.on_epoch_begin(epoch)\n","    \n","    for xb,yb in train_dl:\n","      xb,yb = cb_handler.on_batch_begin(xb,yb)\n","      \n","      out = learn.model(xb)\n","      out = cb_handler.on_loss_begin(out)\n","      \n","      loss = learn.loss_func(out, yb)\n","      \n","      loss, skip_backward = cb_handler.on_backward_begin(loss)\n","      if not skip_backward: loss.backward()\n","      if not cb_handler.on_backward_end(): learn.opt.step()\n","        \n","      if not cb_handler.on_step_end(): learn.opt.zero_grad()\n","      if not cb_handler.on_batch_end(): break\n","        \n","    val_loss, mets = validate(learn.data.valid_dl, model, metrics)\n","    if not cb_handler.on_epoch_end(val_loss, mets): break\n","      \n","  cb_handler.on_train_end()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ceBsPGc8W6nT","colab_type":"text"},"source":["<h2> Let's build a few </h2>\n","\n","I will have 3 examples for us to follow today, OneCycleScheduler, LRFinder, and CSV Logger."]},{"cell_type":"markdown","metadata":{"id":"0DbE30InXD0P","colab_type":"text"},"source":["What is OneCycleScheduler? Based on this [paper](https://arxiv.org/pdf/1803.09820.pdf) by Leslie Smith. \n","From the documentation: \n","\n","\"Create a Callback that handles the hyperparameters settings following the 1cycle policy for learn. lr_max should be picked with the lr_find test. In phase 1, the learning rates goes from lr_max/div_factor to lr_max linearly while the momentum goes from moms[0] to moms[1] linearly. In phase 2, the learning rates follows a cosine annealing from lr_max to 0, as the momentum goes from moms[1] to moms[0] with the same annealing.\"\"\n","\n","https://docs.fast.ai/callbacks.one_cycle.html#OneCycleScheduler\n","\n","https://docs.fast.ai/callbacks.one_cycle.html#What-is-1cycle?\n","\n","\n","* One difference to note: in pytorch to break we pass False, in fastai we pass True "]},{"cell_type":"code","metadata":{"id":"Chp6UIKmiBaY","colab_type":"code","colab":{}},"source":["from fastai.torch_core import *\n","from fastai.basic_data import DataBunch\n","from fastai.callback import *\n","from fastai.basic_train import Learner, LearnerCallback"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6MU7iCfZfB7J","colab_type":"text"},"source":["<h3> OneCycleScheduler</h3>"]},{"cell_type":"code","metadata":{"id":"x4KHdBqKd0NQ","colab_type":"code","colab":{}},"source":["class OneCycleScheduler(LearnerCallback): # We give it a name and say we inherit from the LearnerCallback class\n","  def __init__(self, learn:Learner, lr_max:float, moms:Floats=(0.95,0.85), # pass in learner, a max lr, and momentums\n","               div_factor:float=25., pct_start:float=0.3, final_div:float=None, # pct_start is how far into training we start slowing the speed\n","              tot_epochs:int=None, start_epoch:int=None):\n","    super().__init__(learn)\n","    self.lr_max = lr_max\n","    self.div_factor = div_factor\n","    self.pct_start = pct_start\n","    self.final_div = final_div\n","    \n","    if self.final_div is None: self.final_div = div_factor * 1e4\n","      \n","    self.moms = tuple(listify(moms,2)) # see below what listify is\n","    if is_listy(self.lr_max): self.lr_max = np.array(self.lr_max)\n","      \n","    self.start_epoch, self.tot_epochs = start_epoch, tot_epoch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zB7Z11Dgd2tq","colab_type":"code","colab":{}},"source":["  def steps(self, *steps_cfg:StartOptEnd):\n","    'Build a learning rate schedule for all parameters'\n","    return [Scheduler(step, n_iter, func=func)\n","           for (step, (n_iter,func)) in zip(steps_cfg, self.phases)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gy9MYqFcd3Pe","colab_type":"code","colab":{}},"source":["  def on_train_begin(self, n_epochs:int, epoch:int, **kwargs:Any)->None:\n","    'Initialize the parameters bsed on the schedule above'\n","    res = {'epoch':self.start_epoch} if self.start_epoch is not None else None\n","    self.start_epoch = ifnone(self.start_epoch, epoch)\n","    self.tot_epochs = ifnone(self.tot_epochs, n_epochs)\n","    \n","    n = len(self.learn.data.train_dl) * self.tot_epochs\n","    a1 = int(n*self.pct_start)\n","    a2 = n-a1\n","    \n","    self.phases = ((a1, annealing_cos), (a2, annealing_cos))\n","    low_lr = self.lr_max/self.div_factor # sets our lower LR\n","    \n","    self.lr_scheds = self.steps((low_lr, self.lr_max),\n","                               (self.lr_max, self.lr_max/self.final_div))\n","    \n","    self.moms_scheds = self.steps(self.moms, (self.moms[1], self.moms[0]))\n","    \n","    self.opt = self.learn.opt\n","    self.opt.lr, self.opt.mom = self.lr_scheds[0].start, self.mom_scheds[0].start\n","    self.idx_s = 0\n","    return res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdioqzqJd6jg","colab_type":"code","colab":{}},"source":["  def jump_to_epoch(self, epoch:int)->None:\n","    for _ in range(len(self.learn.data.train_dl) * epoch):\n","      self.on_batch_end(True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"74Q-YhdVeFdz","colab_type":"code","colab":{}},"source":["  def on_batch_end(self, train, **kwargs:Any)->None:\n","    'Take one step forward on the schedule for optim parameters'\n","    if train:\n","      if self.idx_s >= len(self.lr_scheds): return {'stop_training': True, 'stop_epoch': True}\n","      self.opt.lr = self.lr_scheds[self.idx_s].step()\n","      self.opt.mom = self.moms_scheds[self.idx_s].step()\n","      \n","      # When one scheduler is done, move to the next. \n","      # (in one cycle we have two)\n","      \n","      if self.lr_scheds[self.idx_s].is_done:\n","        self.idx_s += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfeYJBMGenO6","colab_type":"code","colab":{}},"source":["   def on_epoch_end(self, epoch, **kwargs:Any)->None:\n","    'Tell the learner to stop training on finish'\n","    if epoch > self.tot_epochs: return {'stop_training': True}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Q-hAitDfFkj","colab_type":"text"},"source":["<h3>LRFinder</h3>"]},{"cell_type":"code","metadata":{"id":"Pd-xvAEhfHyU","colab_type":"code","colab":{}},"source":["class LRFinder(LearnerCallback):\n","  \"Mock training from `start_lr` to `end_lr` for `num_it` iterations.\"\n","  def __init__(self, learn:Learner, start_lr:float=1e-7, end_lr:float=10, \n","               num_it:int=100, stop_div:bool=True):\n","    super().__init__(learn)\n","    self.data = learn.data\n","    self.stop_div = stop_div\n","    self.sched = Scheduler((start_lr, end_lr), num_it, annealing_exp) # annealing exp is a function"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvcpqyVRf4pQ","colab_type":"code","colab":{}},"source":["  def on_train_begin(self, pbar, **kwargs:Any)->None:\n","    'Initialize optimizer and hyperparameters'\n","    setattr(pbar, 'clean_on_interrupt', True)\n","    # pbar.clean_on_interrupt = True\n","    \n","    self.learn.save('tmp')\n","    self.opt = self.learn.opt\n","    self.opt.lr = self.sched.start\n","    self.stop = False\n","    self.best_loss = 0.\n","    return {'skip_validate': True}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lv8cwo2xguZJ","colab_type":"code","colab":{}},"source":["  def on_batch_end(self, iteration:int, smooth_loss:TensorOrNumber, **kwargs:Any)->None:\n","    'Determine if loss is getting exponentially worse quickly'\n","    if iteration == 0 or smooth_loss < self.best_loss: self.best_loss = smooth_loss\n","    \n","    self.opt.lr = self.sched.step() # go to the next LR available in our two steps\n","    \n","    if self.sched.is_done or (self.stop_div and (smooth_loss > 4*self.best_loss or torch.isnan(smooth_loss))):\n","      'Use the smooth loss to decide if stopping since it is less shaky'\n","      return {'stop_epoch': True, 'stop_training': True} # end epoch early and stop the training"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SiN9Cirkherg","colab_type":"code","colab":{}},"source":["  def on_train_end(self, **kwargs:Any)->None:\n","    'Cleanup the weights'\n","    self.learn.load('tmp', purge=False)\n","    if hasattr(self.learn.model, 'reset'): self.learn.model.reset()\n","    for cb in self.callbacks:\n","      if hasattr(cb, 'reset'): cb.reset()\n","    print('LR Finder is complete, type {learner_name}.recorder.plot() to see the graph')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPqKub6HsGCS","colab_type":"text"},"source":["<h2> Accuracy</h2>"]},{"cell_type":"markdown","metadata":{"id":"7O1bW7CDshrs","colab_type":"text"},"source":["<h3>Metric</h3>"]},{"cell_type":"code","metadata":{"id":"rc58u5v1ska1","colab_type":"code","colab":{}},"source":["def accuracy(input:Tensor, targs:Tensor)->Rank0Tensor:\n","    \"Compute accuracy with `targs` when `input` is bs * n_classes.\"\n","    n = targs.shape[0]\n","    input = input.argmax(dim=-1).view(n,-1)\n","    targs = targs.view(n,-1)\n","    return (input==targs).float().mean()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4xXlxuirsZjJ","colab_type":"text"},"source":["<h3> Callback </h3>"]},{"cell_type":"code","metadata":{"id":"kMmW3QYzsNRs","colab_type":"code","colab":{}},"source":["class AverageMetric(Callback):\n","    \"Wrap a `func` in a callback for metrics computation.\"\n","    def __init__(self, func):\n","        # If it's a partial, use func.func\n","        name = getattr(func,'func',func).__name__\n","        self.func, self.name = func, name\n","\n","    def on_epoch_begin(self, **kwargs):\n","        \"Set the inner value to 0.\"\n","        self.val, self.count = 0.,0\n","\n","    def on_batch_end(self, last_output, last_target, **kwargs):\n","        \"Update metric computation with `last_output` and `last_target`.\"\n","        if not is_listy(last_target): last_target=[last_target]\n","        self.count += last_target[0].size(0)\n","        val = self.func(last_output, *last_target)\n","        self.val += last_target[0].size(0) * val.detach().cpu()\n","\n","    def on_epoch_end(self, last_metrics, **kwargs):\n","        \"Set the final result in `last_metrics`.\"\n","        return add_metrics(last_metrics, self.val/self.count)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJuJ0SZ4sN3c","colab_type":"code","colab":{}},"source":["{'last_metrics': last_metrics + [self.val/self.count]}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LoaJ8ZhAbQMw","colab_type":"text"},"source":["# Useful Links"]},{"cell_type":"markdown","metadata":{"id":"oAMW4FN7bZR6","colab_type":"text"},"source":["https://docs.fast.ai/callbacks.html\n","\n","https://docs.fast.ai/callback.html\n","\n","https://forums.fast.ai/t/using-the-callback-system-in-fastai/16216\n"]},{"cell_type":"markdown","metadata":{"id":"4DkMkRmTXc4d","colab_type":"text"},"source":["# Code Examples"]},{"cell_type":"markdown","metadata":{"id":"GBiBDXdPXKpv","colab_type":"text"},"source":["### for _ in range(epoch)"]},{"cell_type":"code","metadata":{"id":"M4LYiUj3XNE5","colab_type":"code","outputId":"9bfe1d8b-b890-4a32-f1e1-71e706aa0192","executionInfo":{"status":"ok","timestamp":1559599322142,"user_tz":300,"elapsed":299,"user":{"displayName":"Zach Mueller","photoUrl":"","userId":"04922536319835678836"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["for _ in range(3):\n","  print('hello')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["hello\n","hello\n","hello\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Dnk5fpjZafV","colab_type":"text"},"source":["### Listify"]},{"cell_type":"code","metadata":{"id":"SGNx5KPAZs8E","colab_type":"code","colab":{}},"source":["from fastai.core import *\n","from fastai.callback import *\n","from fastai.basic_train import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"esfggNDnZdGv","colab_type":"code","outputId":"ceb0b411-2aaa-4166-b5dd-d83bdb09ba2b","executionInfo":{"status":"ok","timestamp":1559633524411,"user_tz":300,"elapsed":270,"user":{"displayName":"Zach Mueller","photoUrl":"","userId":"04922536319835678836"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["moms = (0.95,0.85)\n","listify(moms,2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.95, 0.85]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"iP-0entAZhgh","colab_type":"code","outputId":"6cff9dd5-0ec7-4f24-9c2d-a48aabb64310","executionInfo":{"status":"ok","timestamp":1559633539489,"user_tz":300,"elapsed":469,"user":{"displayName":"Zach Mueller","photoUrl":"","userId":"04922536319835678836"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tuple(listify(moms,2))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.95, 0.85)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"yOmj0ITGZ-R2","colab_type":"text"},"source":["### Is Listy"]},{"cell_type":"code","metadata":{"id":"Bsh5prL3Z_mm","colab_type":"code","outputId":"469739e0-5b81-4b8d-fe04-5da2ed4ef361","executionInfo":{"status":"ok","timestamp":1559633715816,"user_tz":300,"elapsed":445,"user":{"displayName":"Zach Mueller","photoUrl":"","userId":"04922536319835678836"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["moms = listify(slice(0.95,0.85)) # if we pass in a slice as lr_max\n","is_listy(moms)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"HaJobGSRaEVf","colab_type":"code","outputId":"8aa9b1c3-5e7c-42c2-9175-066f8c25ae7d","executionInfo":{"status":"ok","timestamp":1559633760943,"user_tz":300,"elapsed":489,"user":{"displayName":"Zach Mueller","photoUrl":"","userId":"04922536319835678836"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["if is_listy(moms): moms = np.array(moms)\n","print(moms)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[slice(0.95, 0.85, None)]\n"],"name":"stdout"}]}]}